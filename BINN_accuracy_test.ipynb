{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "from binn import Network\n",
    "from binn import BINN\n",
    "from binn import BINNClassifier\n",
    "from binn import BINNExplainer\n",
    "from binn import ImportanceNetwork\n",
    "\n",
    "# utils is  a file from the BINN github repository\n",
    "from util_for_examples import fit_data_matrix_to_network_input, generate_data\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 81\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TM_P1911_190</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TM_P1911_191</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TM_P1911_192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TM_P1911_193</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TM_P1911_194</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>TM_M2012_198</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>TM_M2012_199</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>TM_M2012_200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>TM_M2012_202</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>TM_M2012_203</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample  group\n",
       "0    TM_P1911_190      2\n",
       "1    TM_P1911_191      2\n",
       "2    TM_P1911_192      2\n",
       "3    TM_P1911_193      2\n",
       "4    TM_P1911_194      2\n",
       "..            ...    ...\n",
       "192  TM_M2012_198      2\n",
       "193  TM_M2012_199      2\n",
       "194  TM_M2012_200      2\n",
       "195  TM_M2012_202      2\n",
       "196  TM_M2012_203      2\n",
       "\n",
       "[197 rows x 2 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Import all the data and test train split\n",
    "\n",
    "# Input Data\n",
    "input_data = pd.read_csv(\"aki_data/test_data.tsv\", sep=\"\\t\", )\n",
    "input_data_qm = pd.read_csv(\"aki_data/test_qm.csv\")\n",
    "translation = pd.read_csv(\"aki_data/translation.tsv\", sep=\"\\t\", index_col=0)\n",
    "pathways = pd.read_csv(\"aki_data/pathways.tsv\", sep=\"\\t\")\n",
    "design_matrix = pd.read_csv(\"aki_data/design_matrix.tsv\", sep=\"\\t\")\n",
    "\n",
    "# split into test and train\n",
    "tm_p19_cols = [col for col in input_data_qm.columns if col.startswith(\"TM_P19\")]\n",
    "tm_m2012_cols = [col for col in input_data_qm.columns if col.startswith(\"TM_M2012\")]\n",
    "\n",
    "# Create DataFrames based on these columns\n",
    "test_input_data = input_data_qm[tm_p19_cols]\n",
    "train_input_data = input_data_qm[tm_m2012_cols]\n",
    "first_column = input_data_qm.iloc[:, 0]\n",
    "train_input_data.insert(0, first_column.name, first_column)\n",
    "test_input_data.insert(0, first_column.name, first_column)\n",
    "\n",
    "\n",
    "#design_matrix\n",
    "mask_tm_m2012 = design_matrix['sample'].str.startswith(\"TM_M2012\")\n",
    "mask_tm_p19 = design_matrix['sample'].str.startswith(\"TM_P19\")\n",
    "# Use the mask to create the separate DataFrames\n",
    "test_design_matrix = design_matrix[mask_tm_p19]\n",
    "train_design_matrix = design_matrix[mask_tm_m2012]\n",
    "\n",
    "# check if design matrix has the right amount of subphenotype 1&2 according to paper\n",
    "train_design_matrix\n",
    "group_counts = train_design_matrix['group'].value_counts()\n",
    "count_group_1 = group_counts.get(1, 0)\n",
    "count_group_2 = group_counts.get(2, 0)\n",
    "print(count_group_1,count_group_2)\n",
    "design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the translasions and the pathways\n",
    "\n",
    "shuffled_translation = np.random.permutation(translation['translation'].values)\n",
    "translation[\"translation\"] = shuffled_translation\n",
    "\n",
    "\n",
    "# Permute pathways (doesnt always work) or import the pathway_copy.csc which contains the shuffled child column\n",
    "shuffled_pathways = np.random.permutation(pathways['child'].values)\n",
    "pathways[\"child\"] = shuffled_pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all proteins to one pathway\n",
    "translation[\"translation\"] = \"R-HSA-8941858\"\n",
    "translation\n",
    "# pathways\n",
    "data = {\n",
    "    'parent': ['R-HSA-8941858','R-HSA-8941858','R-HSA-8941858','R-HSA-8941858','R-HSA-8941858','R-HSA-3108214','R-HSA-3108214'],#'R-HSA-8941858', 'R-HSA-9672393', 'R-HSA-1299361', 'R-HSA-2160916'],\n",
    "    'child': ['R-HSA-3108214','R-HSA-3108215','R-HSA-3108216','R-HSA-3108217','R-HSA-3108218','R-HSA-3108215','R-HSA-3108216']#, 'R-HSA-9672393', 'R-HSA-1299361', 'R-HSA-2160916', 'R-HSA-9682706']\n",
    "}\n",
    "\n",
    "pathways_simple = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 0, Average Accuracy 0.5541666746139526, Average Loss: 0.705829938252767\n",
      "Epoch 1, Average Accuracy 0.5222222208976746, Average Loss: 0.73586439092954\n",
      "Epoch 2, Average Accuracy 0.6486111283302307, Average Loss: 0.6653531226846907\n",
      "Epoch 3, Average Accuracy 0.5694444179534912, Average Loss: 0.6711628337701162\n",
      "Epoch 4, Average Accuracy 0.5569444298744202, Average Loss: 0.6845298045211368\n",
      "Epoch 5, Average Accuracy 0.49861112236976624, Average Loss: 0.7170505854818556\n",
      "Epoch 6, Average Accuracy 0.6166666746139526, Average Loss: 0.6582944558726417\n",
      "Epoch 7, Average Accuracy 0.6347222328186035, Average Loss: 0.6145958238177829\n",
      "Epoch 8, Average Accuracy 0.5736110806465149, Average Loss: 0.6413041899601618\n",
      "Epoch 9, Average Accuracy 0.6819444298744202, Average Loss: 0.5791554699341456\n",
      "Epoch 10, Average Accuracy 0.6166666746139526, Average Loss: 0.6517116145955192\n",
      "Epoch 11, Average Accuracy 0.7166666388511658, Average Loss: 0.586608683069547\n",
      "Epoch 12, Average Accuracy 0.6305555701255798, Average Loss: 0.6384069820245107\n",
      "Epoch 13, Average Accuracy 0.7180555462837219, Average Loss: 0.5864559908707937\n",
      "Epoch 14, Average Accuracy 0.6694444417953491, Average Loss: 0.586733674009641\n",
      "Epoch 15, Average Accuracy 0.7708333134651184, Average Loss: 0.5239984343449274\n",
      "Epoch 16, Average Accuracy 0.7597222328186035, Average Loss: 0.5506827433904012\n",
      "Epoch 17, Average Accuracy 0.7111111283302307, Average Loss: 0.5340933352708817\n",
      "Epoch 18, Average Accuracy 0.769444465637207, Average Loss: 0.510129471619924\n",
      "Epoch 19, Average Accuracy 0.7347222566604614, Average Loss: 0.5550821738110648\n",
      "Epoch 20, Average Accuracy 0.7736111283302307, Average Loss: 0.5221093065208859\n",
      "Epoch 21, Average Accuracy 0.7458333373069763, Average Loss: 0.516161753071679\n",
      "Epoch 22, Average Accuracy 0.7833333611488342, Average Loss: 0.5005575170119604\n",
      "Epoch 23, Average Accuracy 0.7805555462837219, Average Loss: 0.5110248360368941\n",
      "Epoch 24, Average Accuracy 0.8041666746139526, Average Loss: 0.43641092793809044\n",
      "Epoch 25, Average Accuracy 0.8083333373069763, Average Loss: 0.46189050045278335\n",
      "Epoch 26, Average Accuracy 0.8500000238418579, Average Loss: 0.40748755054341423\n",
      "Epoch 27, Average Accuracy 0.7527778148651123, Average Loss: 0.5085573643445969\n",
      "Epoch 28, Average Accuracy 0.7916666865348816, Average Loss: 0.4802551070849101\n",
      "Epoch 29, Average Accuracy 0.7388889193534851, Average Loss: 0.5260071224636502\n",
      "Epoch 30, Average Accuracy 0.7986111044883728, Average Loss: 0.48967712620894116\n",
      "Epoch 31, Average Accuracy 0.7847222089767456, Average Loss: 0.46804602444171906\n",
      "Epoch 32, Average Accuracy 0.7708333134651184, Average Loss: 0.5129926403363546\n",
      "Epoch 33, Average Accuracy 0.8402777910232544, Average Loss: 0.4083586566978031\n",
      "Epoch 34, Average Accuracy 0.8263888955116272, Average Loss: 0.4162312497695287\n",
      "Epoch 35, Average Accuracy 0.7902777791023254, Average Loss: 0.4730471728576554\n",
      "Epoch 36, Average Accuracy 0.7847222089767456, Average Loss: 0.4627210696538289\n",
      "Epoch 37, Average Accuracy 0.7944444417953491, Average Loss: 0.49836155275503796\n",
      "Epoch 38, Average Accuracy 0.7486111521720886, Average Loss: 0.494993157684803\n",
      "Epoch 39, Average Accuracy 0.7486111521720886, Average Loss: 0.49537844045294654\n",
      "Epoch 40, Average Accuracy 0.7944444417953491, Average Loss: 0.44060735983981025\n",
      "Epoch 41, Average Accuracy 0.8333333134651184, Average Loss: 0.40182429552078247\n",
      "Epoch 42, Average Accuracy 0.7250000238418579, Average Loss: 0.5444723458753692\n",
      "Epoch 43, Average Accuracy 0.8125, Average Loss: 0.43873682783709633\n",
      "Epoch 44, Average Accuracy 0.8152778148651123, Average Loss: 0.44724416650003856\n",
      "Epoch 45, Average Accuracy 0.8041666746139526, Average Loss: 0.48529453989532256\n",
      "Epoch 46, Average Accuracy 0.8361111283302307, Average Loss: 0.42861582504378426\n",
      "Epoch 47, Average Accuracy 0.7972222566604614, Average Loss: 0.44880740261740154\n",
      "Epoch 48, Average Accuracy 0.8125, Average Loss: 0.4298584817184342\n",
      "Epoch 49, Average Accuracy 0.8263888955116272, Average Loss: 0.43165280669927597\n",
      "dasdf\n",
      "Accuracy of the model on the test data: 87.76%\n",
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 0, Average Accuracy 0.5597221851348877, Average Loss: 0.7053649144040214\n",
      "Epoch 1, Average Accuracy 0.6361110806465149, Average Loss: 0.6318761424885856\n",
      "Epoch 2, Average Accuracy 0.7041666507720947, Average Loss: 0.5703678809934192\n",
      "Epoch 3, Average Accuracy 0.7944444417953491, Average Loss: 0.5218239277601242\n",
      "Epoch 4, Average Accuracy 0.7763888835906982, Average Loss: 0.5231573979059855\n",
      "Epoch 5, Average Accuracy 0.8013889193534851, Average Loss: 0.5388061785035663\n",
      "Epoch 6, Average Accuracy 0.8194444179534912, Average Loss: 0.47859800193044877\n",
      "Epoch 7, Average Accuracy 0.7319444417953491, Average Loss: 0.5420878993140327\n",
      "Epoch 8, Average Accuracy 0.8222222328186035, Average Loss: 0.49565932154655457\n",
      "Epoch 9, Average Accuracy 0.8152778148651123, Average Loss: 0.4759000324540668\n",
      "Epoch 10, Average Accuracy 0.8180555701255798, Average Loss: 0.44011064453257454\n",
      "Epoch 11, Average Accuracy 0.7736111283302307, Average Loss: 0.4883953266673618\n",
      "Epoch 12, Average Accuracy 0.7736111283302307, Average Loss: 0.5113913897011015\n",
      "Epoch 13, Average Accuracy 0.7638888955116272, Average Loss: 0.4747046132882436\n",
      "Epoch 14, Average Accuracy 0.7374999523162842, Average Loss: 0.5217851549386978\n",
      "Epoch 15, Average Accuracy 0.7875000238418579, Average Loss: 0.4850953883594937\n",
      "Epoch 16, Average Accuracy 0.7777777910232544, Average Loss: 0.4758896612458759\n",
      "Epoch 17, Average Accuracy 0.7666666507720947, Average Loss: 0.48186618586381275\n",
      "Epoch 18, Average Accuracy 0.48749998211860657, Average Loss: 0.8950305249955919\n",
      "Epoch 19, Average Accuracy 0.24027776718139648, Average Loss: 1.174790663851632\n",
      "Epoch 20, Average Accuracy 0.2541666626930237, Average Loss: 1.1011909147103627\n",
      "Epoch 21, Average Accuracy 0.22361111640930176, Average Loss: 1.0314942333433363\n",
      "Epoch 22, Average Accuracy 0.3236111104488373, Average Loss: 0.8990804387463464\n",
      "Epoch 23, Average Accuracy 0.30000001192092896, Average Loss: 0.8812746107578278\n",
      "Epoch 24, Average Accuracy 0.3722222149372101, Average Loss: 0.7825463083055284\n",
      "Epoch 25, Average Accuracy 0.42500001192092896, Average Loss: 0.7432764139440324\n",
      "Epoch 26, Average Accuracy 0.4888888895511627, Average Loss: 0.7325879898336198\n",
      "Epoch 27, Average Accuracy 0.5097222328186035, Average Loss: 0.7059144079685211\n",
      "Epoch 28, Average Accuracy 0.5805555582046509, Average Loss: 0.6848063733842638\n",
      "Epoch 29, Average Accuracy 0.5638888478279114, Average Loss: 0.6773781279722849\n",
      "Epoch 30, Average Accuracy 0.625, Average Loss: 0.6612418625089858\n",
      "Epoch 31, Average Accuracy 0.6208333373069763, Average Loss: 0.6585726340611776\n",
      "Epoch 32, Average Accuracy 0.6499999761581421, Average Loss: 0.6487171848615011\n",
      "Epoch 33, Average Accuracy 0.6625000238418579, Average Loss: 0.6341845624976687\n",
      "Epoch 34, Average Accuracy 0.6625000238418579, Average Loss: 0.6304005947377946\n",
      "Epoch 35, Average Accuracy 0.6972222328186035, Average Loss: 0.6033262676662869\n",
      "Epoch 36, Average Accuracy 0.7111111283302307, Average Loss: 0.5950054575999578\n",
      "Epoch 37, Average Accuracy 0.7291666865348816, Average Loss: 0.5945673618051741\n",
      "Epoch 38, Average Accuracy 0.6833333373069763, Average Loss: 0.5932967364788055\n",
      "Epoch 39, Average Accuracy 0.706944465637207, Average Loss: 0.5965256525410546\n",
      "Epoch 40, Average Accuracy 0.6833333373069763, Average Loss: 0.5961697068479326\n",
      "Epoch 41, Average Accuracy 0.7416666746139526, Average Loss: 0.5740150974856483\n",
      "Epoch 42, Average Accuracy 0.7666666507720947, Average Loss: 0.5575926883353127\n",
      "Epoch 43, Average Accuracy 0.7347222566604614, Average Loss: 0.5498727659384409\n",
      "Epoch 44, Average Accuracy 0.7666666507720947, Average Loss: 0.5055577556292216\n",
      "Epoch 45, Average Accuracy 0.7430555820465088, Average Loss: 0.5447660535573959\n",
      "Epoch 46, Average Accuracy 0.8111111521720886, Average Loss: 0.4929541349411011\n",
      "Epoch 47, Average Accuracy 0.8013889193534851, Average Loss: 0.5260449233982298\n",
      "Epoch 48, Average Accuracy 0.7777777910232544, Average Loss: 0.5090502003828684\n",
      "Epoch 49, Average Accuracy 0.7666666507720947, Average Loss: 0.5077367739544975\n",
      "dasdf\n",
      "Accuracy of the model on the test data: 73.47%\n",
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 0, Average Accuracy 0.5125000476837158, Average Loss: 0.7312185449732674\n",
      "Epoch 1, Average Accuracy 0.5569444298744202, Average Loss: 0.7004352741771274\n",
      "Epoch 2, Average Accuracy 0.6347222328186035, Average Loss: 0.6512981139951282\n",
      "Epoch 3, Average Accuracy 0.6138889193534851, Average Loss: 0.6502562065919241\n",
      "Epoch 4, Average Accuracy 0.6416666507720947, Average Loss: 0.6691295935047997\n",
      "Epoch 5, Average Accuracy 0.6833333373069763, Average Loss: 0.6119501243034998\n",
      "Epoch 6, Average Accuracy 0.7652777433395386, Average Loss: 0.5548749019702276\n",
      "Epoch 7, Average Accuracy 0.7916666865348816, Average Loss: 0.5219378372033437\n",
      "Epoch 8, Average Accuracy 0.7708333134651184, Average Loss: 0.5355622983641095\n",
      "Epoch 9, Average Accuracy 0.7777777910232544, Average Loss: 0.5345378783014085\n",
      "Epoch 10, Average Accuracy 0.7208333611488342, Average Loss: 0.543905269768503\n",
      "Epoch 11, Average Accuracy 0.7555555701255798, Average Loss: 0.5578109671672186\n",
      "Epoch 12, Average Accuracy 0.7597222328186035, Average Loss: 0.5579109622372521\n",
      "Epoch 13, Average Accuracy 0.7486111521720886, Average Loss: 0.5378565018375715\n",
      "Epoch 14, Average Accuracy 0.8263888955116272, Average Loss: 0.4990166889296638\n",
      "Epoch 15, Average Accuracy 0.8263888955116272, Average Loss: 0.46657944718996686\n",
      "Epoch 16, Average Accuracy 0.8152778148651123, Average Loss: 0.45890851153267753\n",
      "Epoch 17, Average Accuracy 0.8263888955116272, Average Loss: 0.44416531258159214\n",
      "Epoch 18, Average Accuracy 0.7458333373069763, Average Loss: 0.5223187075720893\n",
      "Epoch 19, Average Accuracy 0.8541666865348816, Average Loss: 0.42767563462257385\n",
      "Epoch 20, Average Accuracy 0.7972222566604614, Average Loss: 0.5033223223355081\n",
      "Epoch 21, Average Accuracy 0.8083333373069763, Average Loss: 0.43185288376278347\n",
      "Epoch 22, Average Accuracy 0.8888888955116272, Average Loss: 0.3803420116504033\n",
      "Epoch 23, Average Accuracy 0.8083333373069763, Average Loss: 0.46733738564782673\n",
      "Epoch 24, Average Accuracy 0.8263888955116272, Average Loss: 0.44120317945877713\n",
      "Epoch 25, Average Accuracy 0.7986111044883728, Average Loss: 0.48659659094280666\n",
      "Epoch 26, Average Accuracy 0.8180555701255798, Average Loss: 0.4607710838317871\n",
      "Epoch 27, Average Accuracy 0.7625000476837158, Average Loss: 0.4923035916354921\n",
      "Epoch 28, Average Accuracy 0.8083333373069763, Average Loss: 0.42531845966974896\n",
      "Epoch 29, Average Accuracy 0.7430555820465088, Average Loss: 0.5333536085155275\n",
      "Epoch 30, Average Accuracy 0.8194444179534912, Average Loss: 0.45153626965151894\n",
      "Epoch 31, Average Accuracy 0.8222222328186035, Average Loss: 0.45711877445379895\n",
      "Epoch 32, Average Accuracy 0.875, Average Loss: 0.334136211209827\n",
      "Epoch 33, Average Accuracy 0.8402777910232544, Average Loss: 0.4166879430413246\n",
      "Epoch 34, Average Accuracy 0.8083333373069763, Average Loss: 0.45991676631901\n",
      "Epoch 35, Average Accuracy 0.8013889193534851, Average Loss: 0.41268659631411236\n",
      "Epoch 36, Average Accuracy 0.8458333611488342, Average Loss: 0.3937768389781316\n",
      "Epoch 37, Average Accuracy 0.7847222089767456, Average Loss: 0.45016758930351997\n",
      "Epoch 38, Average Accuracy 0.7736111283302307, Average Loss: 0.47852910061677295\n",
      "Epoch 39, Average Accuracy 0.8194444179534912, Average Loss: 0.38777091602484387\n",
      "Epoch 40, Average Accuracy 0.8611111044883728, Average Loss: 0.366662983265188\n",
      "Epoch 41, Average Accuracy 0.8361111283302307, Average Loss: 0.39041637380917865\n",
      "Epoch 42, Average Accuracy 0.7902777791023254, Average Loss: 0.4524625730183389\n",
      "Epoch 43, Average Accuracy 0.7777777910232544, Average Loss: 0.46690618991851807\n",
      "Epoch 44, Average Accuracy 0.8402777910232544, Average Loss: 0.3606001337369283\n",
      "Epoch 45, Average Accuracy 0.831944465637207, Average Loss: 0.4168938737776544\n",
      "Epoch 46, Average Accuracy 0.7805555462837219, Average Loss: 0.4663579811652501\n",
      "Epoch 47, Average Accuracy 0.7597222328186035, Average Loss: 0.559812648428811\n",
      "Epoch 48, Average Accuracy 0.875, Average Loss: 0.3647088035941124\n",
      "Epoch 49, Average Accuracy 0.8430555462837219, Average Loss: 0.39518386539485717\n",
      "dasdf\n",
      "Accuracy of the model on the test data: 81.63%\n",
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 0, Average Accuracy 0.5708333253860474, Average Loss: 0.730223662323422\n",
      "Epoch 1, Average Accuracy 0.644444465637207, Average Loss: 0.6466042250394821\n",
      "Epoch 2, Average Accuracy 0.6625000238418579, Average Loss: 0.6153637551599078\n",
      "Epoch 3, Average Accuracy 0.668055534362793, Average Loss: 0.6212528066502677\n",
      "Epoch 4, Average Accuracy 0.6902778148651123, Average Loss: 0.5707867443561554\n",
      "Epoch 5, Average Accuracy 0.7041666507720947, Average Loss: 0.5635556942886777\n",
      "Epoch 6, Average Accuracy 0.7430555820465088, Average Loss: 0.5112176140149435\n",
      "Epoch 7, Average Accuracy 0.75, Average Loss: 0.5767882333861457\n",
      "Epoch 8, Average Accuracy 0.7777777910232544, Average Loss: 0.5312866121530533\n",
      "Epoch 9, Average Accuracy 0.7666666507720947, Average Loss: 0.5347481403085921\n",
      "Epoch 10, Average Accuracy 0.7152777910232544, Average Loss: 0.573026507265038\n",
      "Epoch 11, Average Accuracy 0.7208333611488342, Average Loss: 0.5696546385685602\n",
      "Epoch 12, Average Accuracy 0.7569444179534912, Average Loss: 0.5396774642997317\n",
      "Epoch 13, Average Accuracy 0.8152778148651123, Average Loss: 0.45066048370467293\n",
      "Epoch 14, Average Accuracy 0.7708333134651184, Average Loss: 0.49299048715167576\n",
      "Epoch 15, Average Accuracy 0.8083333373069763, Average Loss: 0.46643663611676955\n",
      "Epoch 16, Average Accuracy 0.8194444179534912, Average Loss: 0.4377908814284537\n",
      "Epoch 17, Average Accuracy 0.7736111283302307, Average Loss: 0.5335838852657212\n",
      "Epoch 18, Average Accuracy 0.8472222089767456, Average Loss: 0.4467666761742698\n",
      "Epoch 19, Average Accuracy 0.7388889193534851, Average Loss: 0.5249896339244313\n",
      "Epoch 20, Average Accuracy 0.75, Average Loss: 0.523974998957581\n",
      "Epoch 21, Average Accuracy 0.8194444179534912, Average Loss: 0.44530994362301296\n",
      "Epoch 22, Average Accuracy 0.7916666865348816, Average Loss: 0.5217737274037467\n",
      "Epoch 23, Average Accuracy 0.7875000238418579, Average Loss: 0.5099975218375524\n",
      "Epoch 24, Average Accuracy 0.7875000238418579, Average Loss: 0.5076895703872045\n",
      "Epoch 25, Average Accuracy 0.7763888835906982, Average Loss: 0.5132906238238016\n",
      "Epoch 26, Average Accuracy 0.8180555701255798, Average Loss: 0.46466761661900413\n",
      "Epoch 27, Average Accuracy 0.8111111521720886, Average Loss: 0.4547155913379457\n",
      "Epoch 28, Average Accuracy 0.8138888478279114, Average Loss: 0.4623945636881722\n",
      "Epoch 29, Average Accuracy 0.8472222089767456, Average Loss: 0.4325060347716014\n",
      "Epoch 30, Average Accuracy 0.7944444417953491, Average Loss: 0.4518030848768022\n",
      "Epoch 31, Average Accuracy 0.8013889193534851, Average Loss: 0.42196784499618745\n",
      "Epoch 32, Average Accuracy 0.7791666388511658, Average Loss: 0.5073161795735359\n",
      "Epoch 33, Average Accuracy 0.8041666746139526, Average Loss: 0.44090525805950165\n",
      "Epoch 34, Average Accuracy 0.8055555820465088, Average Loss: 0.48750947912534076\n",
      "Epoch 35, Average Accuracy 0.7972222566604614, Average Loss: 0.4610083724061648\n",
      "Epoch 36, Average Accuracy 0.7250000238418579, Average Loss: 0.5159204436673058\n",
      "Epoch 37, Average Accuracy 0.8611111044883728, Average Loss: 0.37810997913281125\n",
      "Epoch 38, Average Accuracy 0.8680555820465088, Average Loss: 0.35896675537029904\n",
      "Epoch 39, Average Accuracy 0.8597222566604614, Average Loss: 0.38491068449285293\n",
      "Epoch 40, Average Accuracy 0.7999999523162842, Average Loss: 0.43440402961439556\n",
      "Epoch 41, Average Accuracy 0.7597222328186035, Average Loss: 0.5040853520234426\n",
      "Epoch 42, Average Accuracy 0.8125, Average Loss: 0.42580919878350365\n",
      "Epoch 43, Average Accuracy 0.7916666865348816, Average Loss: 0.4645904675126076\n",
      "Epoch 44, Average Accuracy 0.8541666865348816, Average Loss: 0.38786280900239944\n",
      "Epoch 45, Average Accuracy 0.8125, Average Loss: 0.4131665925184886\n",
      "Epoch 46, Average Accuracy 0.8430555462837219, Average Loss: 0.37341989907953477\n",
      "Epoch 47, Average Accuracy 0.8500000238418579, Average Loss: 0.3792720412214597\n",
      "Epoch 48, Average Accuracy 0.8180555701255798, Average Loss: 0.38819057908323074\n",
      "Epoch 49, Average Accuracy 0.7638888955116272, Average Loss: 0.503341337872876\n",
      "dasdf\n",
      "Accuracy of the model on the test data: 87.76%\n",
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 0, Average Accuracy 0.5652778148651123, Average Loss: 0.780261680483818\n",
      "Epoch 1, Average Accuracy 0.543055534362793, Average Loss: 0.8405815379487144\n",
      "Epoch 2, Average Accuracy 0.5958333611488342, Average Loss: 0.7592567784918679\n",
      "Epoch 3, Average Accuracy 0.6138889193534851, Average Loss: 0.6912422842449613\n",
      "Epoch 4, Average Accuracy 0.6041666865348816, Average Loss: 0.7037296245495478\n",
      "Epoch 5, Average Accuracy 0.7013888955116272, Average Loss: 0.6012709223561816\n",
      "Epoch 6, Average Accuracy 0.7527778148651123, Average Loss: 0.5508469177616967\n",
      "Epoch 7, Average Accuracy 0.7250000238418579, Average Loss: 0.5774851557281282\n",
      "Epoch 8, Average Accuracy 0.7277777791023254, Average Loss: 0.5940895014339023\n",
      "Epoch 9, Average Accuracy 0.7250000238418579, Average Loss: 0.5488020380338033\n",
      "Epoch 10, Average Accuracy 0.7638888955116272, Average Loss: 0.5127393222517438\n",
      "Epoch 11, Average Accuracy 0.7944444417953491, Average Loss: 0.4671332985162735\n",
      "Epoch 12, Average Accuracy 0.7583333253860474, Average Loss: 0.5406817487544484\n",
      "Epoch 13, Average Accuracy 0.47361108660697937, Average Loss: 0.8479853603574965\n",
      "Epoch 14, Average Accuracy 0.32083332538604736, Average Loss: 0.9567098451985253\n",
      "Epoch 15, Average Accuracy 0.29305556416511536, Average Loss: 0.9467506243122948\n",
      "Epoch 16, Average Accuracy 0.7388889193534851, Average Loss: 0.5707759360472361\n",
      "Epoch 17, Average Accuracy 0.6583333611488342, Average Loss: 0.5900749762852987\n",
      "Epoch 18, Average Accuracy 0.6722222566604614, Average Loss: 0.6205991788042916\n",
      "Epoch 19, Average Accuracy 0.7833333611488342, Average Loss: 0.537300545308325\n",
      "Epoch 20, Average Accuracy 0.7708333134651184, Average Loss: 0.5211426864067713\n",
      "Epoch 21, Average Accuracy 0.7430555820465088, Average Loss: 0.5589539607365926\n",
      "Epoch 22, Average Accuracy 0.8083333373069763, Average Loss: 0.48737269474400413\n",
      "Epoch 23, Average Accuracy 0.8152778148651123, Average Loss: 0.4945579535431332\n",
      "Epoch 24, Average Accuracy 0.7416666746139526, Average Loss: 0.5668355888790555\n",
      "Epoch 25, Average Accuracy 0.8361111283302307, Average Loss: 0.46826425360308754\n",
      "Epoch 26, Average Accuracy 0.7944444417953491, Average Loss: 0.497238877746794\n",
      "Epoch 27, Average Accuracy 0.7458333373069763, Average Loss: 0.522950741979811\n",
      "Epoch 28, Average Accuracy 0.8152778148651123, Average Loss: 0.47834419045183396\n",
      "Epoch 29, Average Accuracy 0.39722222089767456, Average Loss: 1.0049456937445536\n",
      "Epoch 30, Average Accuracy 0.25138890743255615, Average Loss: 1.1321045325862036\n",
      "Epoch 31, Average Accuracy 0.34583333134651184, Average Loss: 1.0009051362673442\n",
      "Epoch 32, Average Accuracy 0.31388890743255615, Average Loss: 0.9309955504205492\n",
      "Epoch 33, Average Accuracy 0.3333333432674408, Average Loss: 0.8811653157075247\n",
      "Epoch 34, Average Accuracy 0.38749998807907104, Average Loss: 0.8146046433183882\n",
      "Epoch 35, Average Accuracy 0.4583333432674408, Average Loss: 0.7665187054210238\n",
      "Epoch 36, Average Accuracy 0.5833333134651184, Average Loss: 0.677297732896275\n",
      "Epoch 37, Average Accuracy 0.5638888478279114, Average Loss: 0.6694218433565564\n",
      "Epoch 38, Average Accuracy 0.6458333134651184, Average Loss: 0.6419793417056402\n",
      "Epoch 39, Average Accuracy 0.6694444417953491, Average Loss: 0.6351038018862406\n",
      "Epoch 40, Average Accuracy 0.5958333611488342, Average Loss: 0.6589347124099731\n",
      "Epoch 41, Average Accuracy 0.6000000238418579, Average Loss: 0.685003442896737\n",
      "Epoch 42, Average Accuracy 0.7222222089767456, Average Loss: 0.605424208773507\n",
      "Epoch 43, Average Accuracy 0.6305555701255798, Average Loss: 0.6642694804403517\n",
      "Epoch 44, Average Accuracy 0.7000000476837158, Average Loss: 0.6051037477122413\n",
      "Epoch 45, Average Accuracy 0.7833333611488342, Average Loss: 0.553394079208374\n",
      "Epoch 46, Average Accuracy 0.7291666865348816, Average Loss: 0.5717840145031611\n",
      "Epoch 47, Average Accuracy 0.7180555462837219, Average Loss: 0.5875860734118356\n",
      "Epoch 48, Average Accuracy 0.730555534362793, Average Loss: 0.5568990028566785\n",
      "Epoch 49, Average Accuracy 0.7597222328186035, Average Loss: 0.5538098497523202\n",
      "dasdf\n",
      "Accuracy of the model on the test data: 75.51%\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "counter = 0\n",
    "for run in range(5):\n",
    "    \n",
    "    counter +=1 \n",
    "    network = Network(\n",
    "        input_data    =   input_data_qm, # use the preprocessed data\n",
    "        pathways      =   pathways_simple,\n",
    "        mapping       =   translation,\n",
    "        input_data_column = \"Protein\", # This is the default value\n",
    "        source_column = \"parent\", # defined by our pathways-file\n",
    "        target_column = \"child\",\n",
    "        subset_pathways = True # This is the default value\n",
    "    )\n",
    "\n",
    "    binn = BINN(\n",
    "        network=network,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        validate=False,\n",
    "        residual=False,\n",
    "        device=\"cpu\",\n",
    "        learning_rate=0.001\n",
    "        # defaults\n",
    "        # activation = \"tanh\"\n",
    "        # weight = torch.tensor([1, 1])\n",
    "        # scheduler = \"plateau\"\n",
    "        # optimizer = \"adam\"\n",
    "        # n_outputs = 2\n",
    "    )\n",
    "\n",
    "    ### Training with all train data no CV\n",
    "\n",
    "    protein_matrix = fit_data_matrix_to_network_input(train_input_data, features=network.inputs)\n",
    "    X, y = generate_data(protein_matrix, design_matrix=train_design_matrix)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32, device=binn.device),\n",
    "        torch.tensor(y, dtype=torch.torch.long, device=binn.device),\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "    # You can also train with a standard PyTorch train loop \n",
    "\n",
    "    optimizer = binn.configure_optimizers()[0][0]\n",
    "\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        binn.train() \n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(binn.device)\n",
    "            targets = targets.to(binn.device).type(torch.LongTensor)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = binn(inputs).to(binn.device)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += torch.sum(torch.argmax(outputs, axis=1) == targets) / len(targets)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_accuracy = total_accuracy / len(dataloader)\n",
    "        print(f'Epoch {epoch}, Average Accuracy {avg_accuracy}, Average Loss: {avg_loss}')\n",
    "\n",
    "\n",
    "    # Using the test manually\n",
    "\n",
    "    # and test_loader is your DataLoader for the test data\n",
    "    protein_matrix_test = fit_data_matrix_to_network_input(test_input_data, features=network.inputs)\n",
    "    # Assuming generate_data() returns the entire dataset\n",
    "    X_test, y_test = generate_data(protein_matrix_test, design_matrix=test_design_matrix)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # tryout long instead of int16\n",
    "\n",
    "    # Create a DataLoader for your test data\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8)  # Adjust batch size as needed\n",
    "    # Set the model to evaluation mode\n",
    "    binn.eval()\n",
    "    \n",
    "    # Initialize variables to track accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # No need to track gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Move data to the same device as your model\n",
    "            inputs = inputs.to(binn.device)\n",
    "            labels = labels.to(binn.device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = binn(inputs)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.22448979591836 5.9709138115623634\n"
     ]
    }
   ],
   "source": [
    "# 30 samples on 50 epochs\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(mean_accuracy,std_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import synthetic_data as sync_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "from binn import Network\n",
    "from binn import BINN\n",
    "from binn import BINNClassifier\n",
    "from binn import BINNExplainer\n",
    "from binn import ImportanceNetwork\n",
    "\n",
    "# utils is  a file from the BINN github repository\n",
    "from util_for_examples import fit_data_matrix_to_network_input, generate_data\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9000392397938404\n",
      "-1\n",
      "1.0144569349470727\n",
      "111\n",
      "0.9823503670950573\n",
      "-1\n",
      "-0.4151301923577571\n",
      "-1\n",
      "1.5318529604652769\n",
      "111\n",
      "-1.0584952847804987\n",
      "111\n",
      "-0.6756569446117376\n",
      "-1\n",
      "-7.767671961112741\n",
      "111\n",
      "-5.22431228045784\n",
      "111\n",
      "0.5469597735236577\n",
      "-1\n",
      "-6.793826514714235\n",
      "111\n",
      "1.6266329943757771\n",
      "111\n",
      "0.35513989659731543\n",
      "-1\n",
      "0.27298750150784457\n",
      "-1\n",
      "0.8137070201567544\n",
      "-1\n",
      "0.9023657786163095\n",
      "-1\n",
      "-4.26535229620022\n",
      "111\n",
      "1.5643468194856944\n",
      "111\n",
      "-0.0052686709239992404\n",
      "-1\n",
      "-2.9362751133206575\n",
      "111\n",
      "-7.808176251905342\n",
      "111\n",
      "-0.7670432522226862\n",
      "-1\n",
      "-0.9530603095973521\n",
      "-1\n",
      "0.9314869251920663\n",
      "-1\n",
      "0.3173277700674413\n",
      "-1\n",
      "-2.1654752871568537\n",
      "111\n",
      "0.9069854581080896\n",
      "-1\n",
      "27.347042141486344\n",
      "111\n",
      "1.3869582983922628\n",
      "111\n",
      "1.4735364728780969\n",
      "111\n",
      "1.4551671387637228\n",
      "111\n",
      "1.1270711953099717\n",
      "111\n",
      "0.9118368441458862\n",
      "-1\n",
      "1.0124724589593672\n",
      "111\n",
      "0.7429625029070933\n",
      "-1\n",
      "1.6278538756616105\n",
      "111\n",
      "-4.885465687096948\n",
      "111\n",
      "0.9031996912351726\n",
      "-1\n",
      "0.8930880716641261\n",
      "-1\n",
      "-6.34596240344116\n",
      "111\n",
      "1.4743752549772433\n",
      "111\n",
      "-4.920845978426838\n",
      "111\n",
      "1.4280326445514298\n",
      "111\n",
      "0.8897587777234441\n",
      "-1\n",
      "-1.3684158523505447\n",
      "111\n",
      "1.191849252529205\n",
      "111\n",
      "-1.3940638195272117\n",
      "111\n",
      "1.42005619096744\n",
      "111\n",
      "-5.488205880776203\n",
      "111\n",
      "0.8584651980293956\n",
      "-1\n",
      "1.0328880498252953\n",
      "111\n",
      "1.61236654850529\n",
      "111\n",
      "-8.052296432079848\n",
      "111\n",
      "1.6029068872381698\n",
      "111\n",
      "1.1467568026632529\n",
      "111\n",
      "-1.4271760378321945\n",
      "111\n",
      "-0.8769839298094134\n",
      "-1\n",
      "-0.990646548858753\n",
      "-1\n",
      "-4.149348365125885\n",
      "111\n",
      "-0.6053076158700111\n",
      "-1\n",
      "-5.591128126082251\n",
      "111\n",
      "-1.7636562656265822\n",
      "111\n",
      "-0.7072128566925023\n",
      "-1\n",
      "0.917287849215222\n",
      "-1\n",
      "1.5589827598829893\n",
      "111\n",
      "0.8928587985802927\n",
      "-1\n",
      "0.9001935542899575\n",
      "-1\n",
      "0.9219863274947684\n",
      "-1\n",
      "-6.944895837475914\n",
      "111\n",
      "-1.0654376927530405\n",
      "111\n",
      "-5.146648029321245\n",
      "111\n",
      "0.21589162280402607\n",
      "-1\n",
      "0.9679108860695285\n",
      "-1\n",
      "1.4921876660679938\n",
      "111\n",
      "0.3868341168165853\n",
      "-1\n",
      "0.04752710777526974\n",
      "-1\n",
      "0.8899379504031215\n",
      "-1\n",
      "-7.7912690823583155\n",
      "111\n",
      "0.8950370508058629\n",
      "-1\n",
      "0.8963330844248554\n",
      "-1\n",
      "0.8726517862843423\n",
      "-1\n",
      "0.27189790926788887\n",
      "-1\n",
      "-1.0219592315060473\n",
      "111\n",
      "1.5056109325573546\n",
      "111\n",
      "0.7112685703885334\n",
      "-1\n",
      "1.1585114261906775\n",
      "111\n",
      "-0.6818443156294149\n",
      "-1\n",
      "0.8505098790845962\n",
      "-1\n",
      "-0.14847706244754125\n",
      "-1\n",
      "0.5829681472776218\n",
      "-1\n",
      "0.8064829653232981\n",
      "-1\n",
      "0.8445435033905039\n",
      "-1\n",
      "-0.06412129832143754\n",
      "-1\n",
      "9.216209959081407\n",
      "111\n",
      "-3.4121071364719673\n",
      "111\n",
      "-6.465599532883133\n",
      "111\n",
      "-7.442196616337493\n",
      "111\n",
      "0.42391075350963403\n",
      "-1\n",
      "0.6718334587908803\n",
      "-1\n",
      "-6.094651789396642\n",
      "111\n",
      "0.8945645988450031\n",
      "-1\n",
      "0.8459844219927923\n",
      "-1\n",
      "1.622109239706303\n",
      "111\n",
      "-1.0663933873195777\n",
      "111\n",
      "0.894508742263161\n",
      "-1\n",
      "0.7171421650075385\n",
      "-1\n",
      "1.459652601504729\n",
      "111\n",
      "-0.5649985711518501\n",
      "-1\n",
      "-6.712744484047693\n",
      "111\n",
      "1.351413998147555\n",
      "111\n",
      "1.38579356457093\n",
      "111\n",
      "0.6494588258766005\n",
      "-1\n",
      "1.6282114616727406\n",
      "111\n",
      "1.1980356722882248\n",
      "111\n",
      "0.9106078720104329\n",
      "-1\n",
      "1.576000705457251\n",
      "111\n",
      "-0.26120432175254427\n",
      "-1\n",
      "29.891748964775765\n",
      "111\n",
      "-0.7182027138343476\n",
      "-1\n",
      "-3.2359184162924235\n",
      "111\n",
      "1.5436555543306056\n",
      "111\n",
      "-1.2121547634108214\n",
      "111\n",
      "-0.3668115214077874\n",
      "-1\n",
      "1.20963517483487\n",
      "111\n",
      "1.2500720368053304\n",
      "111\n",
      "1.6216675543909784\n",
      "111\n",
      "1.4939236968002982\n",
      "111\n",
      "0.34110692527383407\n",
      "-1\n",
      "0.8942592567871084\n",
      "-1\n",
      "-7.028073456654168\n",
      "111\n",
      "-1.0449062030401632\n",
      "111\n",
      "1.272771801784361\n",
      "111\n",
      "-8.093407112083883\n",
      "111\n",
      "-0.21070517210803172\n",
      "-1\n",
      "1.628412232673753\n",
      "111\n",
      "-8.07319773746446\n",
      "111\n",
      "1.5976730426966776\n",
      "111\n",
      "14.82706498202952\n",
      "111\n",
      "-0.6601251025391847\n",
      "-1\n",
      "-7.913112977619129\n",
      "111\n",
      "-77.21985402664733\n",
      "111\n",
      "0.8338678580797168\n",
      "-1\n",
      "-8.020987978877525\n",
      "111\n",
      "1.5147731580089958\n",
      "111\n",
      "-7.370303318186729\n",
      "111\n",
      "1.502945283354567\n",
      "111\n",
      "-0.09675509527259658\n",
      "-1\n",
      "-1.2620039706164357\n",
      "111\n",
      "1.5601962858694054\n",
      "111\n",
      "0.14868269160110473\n",
      "-1\n",
      "-1.5223100064572463\n",
      "111\n",
      "-1.4503461204902632\n",
      "111\n",
      "-6.314177235031683\n",
      "111\n",
      "-8.094161527544836\n",
      "111\n",
      "-1.1073963605947608\n",
      "111\n",
      "-0.39112564940401373\n",
      "-1\n",
      "-4.400745158766785\n",
      "111\n",
      "0.5947450810460595\n",
      "-1\n",
      "-0.8757990466998469\n",
      "-1\n",
      "0.7702450164302612\n",
      "-1\n",
      "-4.272444956178958\n",
      "111\n",
      "3.3290935541450395\n",
      "111\n",
      "0.90374379650465\n",
      "-1\n",
      "0.8999835066433292\n",
      "-1\n",
      "-8.046467063272612\n",
      "111\n",
      "-0.9880912342706141\n",
      "-1\n",
      "-4.335194172909183\n",
      "111\n",
      "0.488860463192949\n",
      "-1\n",
      "0.9517127020814037\n",
      "-1\n",
      "0.9116613660906413\n",
      "-1\n",
      "-8.003393031792353\n",
      "111\n",
      "-4.055123965267001\n",
      "111\n",
      "1.4097560031238754\n",
      "111\n",
      "0.8963976988275296\n",
      "-1\n",
      "-0.9149162553375373\n",
      "-1\n",
      "-0.11854506503207703\n",
      "-1\n",
      "0.45031296879901284\n",
      "-1\n",
      "0.9013656795653329\n",
      "-1\n",
      "-7.997781182667158\n",
      "111\n",
      "1.0310325811947654\n",
      "111\n",
      "41.10631651810592\n",
      "111\n",
      "6.866680857600065\n",
      "111\n",
      "0.21970992471008022\n",
      "-1\n",
      "0.8993852027894749\n",
      "-1\n",
      "-3.5887792126647886\n",
      "111\n",
      "1.3467576317171481\n",
      "111\n",
      "1.3726592834859\n",
      "111\n",
      "-0.7121374391636034\n",
      "-1\n",
      "0.8983515905314408\n",
      "-1\n",
      "0.8966811747685736\n",
      "-1\n",
      "1.528534532370213\n",
      "111\n",
      "0.08785031818093617\n",
      "-1\n",
      "0.898304727403838\n",
      "-1\n",
      "0.6035965640077783\n",
      "-1\n",
      "1.5956132826216638\n",
      "111\n",
      "1.6184734042733377\n",
      "111\n",
      "-3.3268479342128754\n",
      "111\n",
      "-0.4228462150806289\n",
      "-1\n",
      "-1.0045054009088727\n",
      "111\n",
      "0.8950635193359053\n",
      "-1\n",
      "1.5314920891058144\n",
      "111\n",
      "-7.014287174183561\n",
      "111\n",
      "-5.558990644622394\n",
      "111\n",
      "-3.068180402366791\n",
      "111\n",
      "1.004716877930609\n",
      "111\n",
      "-4.176585745345476\n",
      "111\n",
      "0.6486618476440256\n",
      "-1\n",
      "0.8095834808227236\n",
      "-1\n",
      "-1.3047570600952154\n",
      "111\n",
      "1.4141725696775336\n",
      "111\n",
      "1.189686294965675\n",
      "111\n",
      "16.773904445709327\n",
      "111\n",
      "0.897736525183739\n",
      "-1\n",
      "1.6245207102187997\n",
      "111\n",
      "-8.093393308734193\n",
      "111\n",
      "1.534768823248103\n",
      "111\n",
      "-0.31076014901366966\n",
      "-1\n",
      "0.9178274907084141\n",
      "-1\n",
      "-3.0511907162704444\n",
      "111\n",
      "-1.0663417080884625\n",
      "111\n",
      "-4.7201707027525925\n",
      "111\n",
      "-8.067342210184215\n",
      "111\n",
      "0.29197144325672075\n",
      "-1\n",
      "-0.8900385994526385\n",
      "-1\n",
      "1.5100806969189644\n",
      "111\n",
      "0.3851391164618897\n",
      "-1\n",
      "0.9014935160014685\n",
      "-1\n",
      "0.6172385613492896\n",
      "-1\n",
      "-0.9241238755484982\n",
      "-1\n",
      "1.2263098948253892\n",
      "111\n",
      "-0.1570665022359562\n",
      "-1\n",
      "1.5974357684929261\n",
      "111\n",
      "1.149398038333186\n",
      "111\n",
      "0.8975336604933445\n",
      "-1\n",
      "58.15026391799028\n",
      "111\n",
      "0.7733140407214495\n",
      "-1\n",
      "1.0489970873498298\n",
      "111\n",
      "-0.37313626590193416\n",
      "-1\n",
      "1.0670943782950149\n",
      "111\n",
      "3.4810755255503203\n",
      "111\n",
      "-0.2914954536399607\n",
      "-1\n",
      "-4.008945141904273\n",
      "111\n",
      "51.64179860098815\n",
      "111\n",
      "0.9556166405634408\n",
      "-1\n",
      "-6.732012621296315\n",
      "111\n",
      "-3.8839711130487093\n",
      "111\n",
      "1.3305924997312824\n",
      "111\n",
      "-8.090920621860876\n",
      "111\n",
      "-0.059346667981513966\n",
      "-1\n",
      "1.6064803516779655\n",
      "111\n",
      "0.9028284174598612\n",
      "-1\n",
      "-0.40561882308911534\n",
      "-1\n",
      "0.5071507632801507\n",
      "-1\n",
      "1.5368276609927194\n",
      "111\n",
      "1.415837097371094\n",
      "111\n",
      "-7.461457042031885\n",
      "111\n",
      "0.5783543754595122\n",
      "-1\n",
      "-3.819861908433322\n",
      "111\n",
      "-0.6932643530073064\n",
      "-1\n",
      "0.6105974334883943\n",
      "-1\n",
      "0.8999574866432902\n",
      "-1\n",
      "1.0165060759337379\n",
      "111\n",
      "1.6080563118560285\n",
      "111\n",
      "0.08931550426554777\n",
      "-1\n",
      "-4.131264581059331\n",
      "111\n",
      "1.61945608876746\n",
      "111\n",
      "1.6114580273205825\n",
      "111\n",
      "-8.034204477225813\n",
      "111\n",
      "-7.099379732874272\n",
      "111\n",
      "1.0581436241209647\n",
      "111\n",
      "-7.80944790922699\n",
      "111\n",
      "-8.089916517466174\n",
      "111\n",
      "1.6088965374021929\n",
      "111\n",
      "-2.155845062108093\n",
      "111\n",
      "-0.27148028776307914\n",
      "-1\n",
      "0.21333853671527203\n",
      "-1\n",
      "0.9019376027057409\n",
      "-1\n",
      "0.9547372772388628\n",
      "-1\n",
      "1.0800689344550987\n",
      "111\n",
      "0.9147972799800146\n",
      "-1\n",
      "0.22910345173651736\n",
      "-1\n",
      "1.2700566042381507\n",
      "111\n",
      "37.321209014195695\n",
      "111\n",
      "-8.091070216719258\n",
      "111\n",
      "-8.094211293486014\n",
      "111\n",
      "-0.34313184600038255\n",
      "-1\n",
      "-7.864984456655377\n",
      "111\n",
      "2.7622594754491523\n",
      "111\n",
      "0.9122213740140781\n",
      "-1\n",
      "-3.9314522859219165\n",
      "111\n",
      "0.707065178689026\n",
      "-1\n",
      "1.6177111505390127\n",
      "111\n",
      "-5.888646327802171\n",
      "111\n",
      "-6.550346436698992\n",
      "111\n",
      "1.4111216113274452\n",
      "111\n",
      "1.6276800700183087\n",
      "111\n",
      "1.4429878472765894\n",
      "111\n",
      "1.29625971349898\n",
      "111\n",
      "-0.44215335447721993\n",
      "-1\n",
      "14.589075276822154\n",
      "111\n",
      "1.5074751295569844\n",
      "111\n",
      "-7.8328621095105895\n",
      "111\n",
      "0.7756870555895583\n",
      "-1\n",
      "-0.9019491749900156\n",
      "-1\n",
      "-86.65614908933344\n",
      "111\n",
      "-4.667700238065255\n",
      "111\n",
      "-0.9352799259818644\n",
      "-1\n",
      "-5.831492534214193\n",
      "111\n",
      "0.7616308245111465\n",
      "-1\n",
      "1.029576438903366\n",
      "111\n",
      "0.35766204665000106\n",
      "-1\n",
      "0.5291241516960714\n",
      "-1\n",
      "-5.235964528536999\n",
      "111\n",
      "1.0972039737888588\n",
      "111\n",
      "1.4499466062988564\n",
      "111\n",
      "0.9025536849466999\n",
      "-1\n",
      "1.064860397314517\n",
      "111\n",
      "11.403041189422359\n",
      "111\n",
      "1.627566473147429\n",
      "111\n",
      "0.8967907146468299\n",
      "-1\n",
      "1.0070567843108105\n",
      "111\n",
      "0.7558098944849685\n",
      "-1\n",
      "0.589032535220148\n",
      "-1\n",
      "-1.0649418683477023\n",
      "111\n",
      "0.5035238521775665\n",
      "-1\n",
      "9.377005809456746\n",
      "111\n",
      "-4.005398767242711\n",
      "111\n",
      "-1.0155276834887188\n",
      "111\n",
      "1.1340280578663409\n",
      "111\n",
      "1.6286095381308132\n",
      "111\n",
      "2.5232686340684487\n",
      "111\n",
      "-1.460657563169019\n",
      "111\n",
      "4.202915495096194\n",
      "111\n",
      "0.9751780701739999\n",
      "-1\n",
      "0.5474040491690539\n",
      "-1\n",
      "0.926077937102252\n",
      "-1\n",
      "-7.904006252738448\n",
      "111\n",
      "-1.765132121589664\n",
      "111\n",
      "-0.8335919328984681\n",
      "-1\n",
      "1.3368847005607194\n",
      "111\n",
      "-6.284459830304069\n",
      "111\n",
      "0.8995105597317749\n",
      "-1\n",
      "21.364827644145343\n",
      "111\n",
      "-6.767276291755535\n",
      "111\n",
      "-0.963440561014336\n",
      "-1\n",
      "1.2687000781447209\n",
      "111\n",
      "26.39084923615363\n",
      "111\n",
      "1.6284829256309248\n",
      "111\n",
      "-5.620476152662121\n",
      "111\n",
      "-5.889629521873539\n",
      "111\n",
      "-1.7104234474125886\n",
      "111\n",
      "0.9290632258699268\n",
      "-1\n",
      "1.600610797370102\n",
      "111\n",
      "0.49259650542337957\n",
      "-1\n",
      "1.1589295053627777\n",
      "111\n",
      "0.9025231351801332\n",
      "-1\n",
      "-4.940479471458366\n",
      "111\n",
      "-7.955745272971543\n",
      "111\n",
      "-0.2287880253912004\n",
      "-1\n",
      "0.8934427393555116\n",
      "-1\n",
      "0.1676060905143149\n",
      "-1\n",
      "0.8418751441428132\n",
      "-1\n",
      "0.9037341502501446\n",
      "-1\n",
      "0.89485837401754\n",
      "-1\n",
      "-6.072417191692397\n",
      "111\n",
      "-7.074987575865853\n",
      "111\n",
      "0.6821687778110167\n",
      "-1\n",
      "-5.864110693023009\n",
      "111\n",
      "1.4681324309876735\n",
      "111\n",
      "-6.8685689965519146\n",
      "111\n",
      "0.8941631412229394\n",
      "-1\n",
      "1.5143324611632272\n",
      "111\n",
      "0.5538470404909619\n",
      "-1\n",
      "1.2260367726116246\n",
      "111\n",
      "-7.606936002062299\n",
      "111\n",
      "-7.65250395658447\n",
      "111\n",
      "-8.056345326168355\n",
      "111\n",
      "-7.896616508946225\n",
      "111\n",
      "-4.044130938617467\n",
      "111\n",
      "-7.893186488288596\n",
      "111\n",
      "1.1195237995154925\n",
      "111\n",
      "1.548924767921835\n",
      "111\n",
      "1.3509294310087194\n",
      "111\n",
      "-7.237004012218396\n",
      "111\n",
      "-4.997875645955914\n",
      "111\n",
      "0.28631468265678506\n",
      "-1\n",
      "-7.3237932293507875\n",
      "111\n",
      "0.9134996877686111\n",
      "-1\n",
      "24.532722703390398\n",
      "111\n",
      "-7.205952390979467\n",
      "111\n",
      "1.4961958593277194\n",
      "111\n",
      "3.705040038809902\n",
      "111\n",
      "-0.09189759548686771\n",
      "-1\n",
      "-7.463764407262561\n",
      "111\n",
      "1.5611198865158737\n",
      "111\n",
      "1.0416756279327344\n",
      "111\n",
      "-7.799112135895082\n",
      "111\n",
      "-4.313862132321984\n",
      "111\n",
      "0.9017295585342993\n",
      "-1\n",
      "-4.174809618113441\n",
      "111\n",
      "-1.226886950586953\n",
      "111\n",
      "1.3625095419815703\n",
      "111\n",
      "1.428991392950249\n",
      "111\n",
      "-7.671244720165348\n",
      "111\n",
      "1.0364811773002787\n",
      "111\n",
      "0.8945613690139095\n",
      "-1\n",
      "1.6280960799135809\n",
      "111\n",
      "-0.9774822359545624\n",
      "-1\n",
      "-6.639802163738016\n",
      "111\n",
      "-4.353380136130601\n",
      "111\n",
      "-1.7376401082241673\n",
      "111\n",
      "-6.620187486043854\n",
      "111\n",
      "0.493134244147165\n",
      "-1\n",
      "-3.9973533909352317\n",
      "111\n",
      "-1.05126632955371\n",
      "111\n",
      "1.6015337016985254\n",
      "111\n",
      "-6.481803599040245\n",
      "111\n",
      "-4.889134641782006\n",
      "111\n",
      "0.5837187449652749\n",
      "-1\n",
      "0.9067309908199576\n",
      "-1\n",
      "1.5477828126596924\n",
      "111\n",
      "-2.079589900203543\n",
      "111\n",
      "0.04087191122269415\n",
      "-1\n",
      "-0.43394035115418117\n",
      "-1\n",
      "-0.9405866767913588\n",
      "-1\n",
      "1.6286178837263392\n",
      "111\n",
      "28.7828201158523\n",
      "111\n",
      "1.1551934534956914\n",
      "111\n",
      "1.7834708860574904\n",
      "111\n",
      "-2.147491286994155\n",
      "111\n",
      "0.442656330489808\n",
      "-1\n",
      "-5.3834464144234975\n",
      "111\n",
      "-7.890828050800872\n",
      "111\n",
      "1.6263822598271973\n",
      "111\n",
      "0.895134114971033\n",
      "-1\n",
      "59.64738126319403\n",
      "111\n",
      "1.5456322360196808\n",
      "111\n",
      "1.2115876177801594\n",
      "111\n",
      "1.4809281026152632\n",
      "111\n",
      "1.4684245086341594\n",
      "111\n",
      "29.70823670782025\n",
      "111\n",
      "0.832122275498252\n",
      "-1\n",
      "1.4044149795891956\n",
      "111\n",
      "0.9281084653251418\n",
      "-1\n",
      "0.9369211658121908\n",
      "-1\n",
      "-2.763227476751162\n",
      "111\n",
      "0.8595189040924984\n",
      "-1\n",
      "-7.529670797752464\n",
      "111\n",
      "-0.30739194287088795\n",
      "-1\n",
      "11.8419505006478\n",
      "111\n",
      "-6.832130597762453\n",
      "111\n",
      "0.027869507754079332\n",
      "-1\n",
      "0.8950424984670137\n",
      "-1\n",
      "-8.089797328592837\n",
      "111\n",
      "12.880818388798145\n",
      "111\n",
      "-7.5274369536337185\n",
      "111\n",
      "0.17118330913940016\n",
      "-1\n",
      "0.8987032546933817\n",
      "-1\n",
      "66.58266508773717\n",
      "111\n",
      "-3.831310185117073\n",
      "111\n",
      "-7.2420092402301055\n",
      "111\n",
      "0.9645690406817885\n",
      "-1\n",
      "-0.7700808970540849\n",
      "-1\n",
      "1.1527738929263969\n",
      "111\n",
      "0.6935884766457225\n",
      "-1\n",
      "1.4019030299493598\n",
      "111\n",
      "0.9056728315593929\n",
      "-1\n",
      "-0.18136281021276632\n",
      "-1\n",
      "0.9225932709647289\n",
      "-1\n",
      "-4.684246338602794\n",
      "111\n",
      "0.784716659598426\n",
      "-1\n",
      "-8.042814291373448\n",
      "111\n",
      "0.9029361589768155\n",
      "-1\n",
      "-7.460849810587661\n",
      "111\n",
      "-7.950999802193113\n",
      "111\n",
      "3.4393444273917773\n",
      "111\n",
      "1.1076481239503435\n",
      "111\n",
      "-0.34917595341695884\n",
      "-1\n",
      "0.590498912564147\n",
      "-1\n",
      "1.6266857593690849\n",
      "111\n",
      "34.63182815554726\n",
      "111\n",
      "1.628439833253697\n",
      "111\n",
      "1.0422856822593691\n",
      "111\n",
      "0.022987754905243022\n",
      "-1\n",
      "-0.7043392093511834\n",
      "-1\n",
      "-7.41967294162101\n",
      "111\n",
      "-5.776502686285339\n",
      "111\n",
      "-1.431462341048041\n",
      "111\n",
      "0.528815851972219\n",
      "-1\n",
      "-8.069542763523247\n",
      "111\n",
      "-2.635089985410839\n",
      "111\n",
      "-0.3877895985122014\n",
      "-1\n",
      "0.9010950702404497\n",
      "-1\n",
      "0.7383144902756137\n",
      "-1\n",
      "0.8911368343303838\n",
      "-1\n",
      "1.5469861778996223\n",
      "111\n",
      "0.49534113885917674\n",
      "-1\n",
      "-2.192921719722933\n",
      "111\n",
      "1.565026121365491\n",
      "111\n",
      "-0.3312508436424194\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "qm_matrix_synthetic,design_matrix_synthetic,translations_synthetic,parent_child_df,key_genes,key_pathways = sync_data.create_synthetic_data()\n",
    "\n",
    "total_columns = len(qm_matrix_synthetic.columns) - 1\n",
    "num_test_columns = int(total_columns * 0.2)\n",
    "\n",
    "column_indices = list(range(1, len(qm_matrix_synthetic.columns)))\n",
    "\n",
    "# Randomly select column indices for the test set\n",
    "np.random.seed(42)  # For reproducibility\n",
    "test_column_indices = np.random.choice(column_indices, size=num_test_columns, replace=False)\n",
    "\n",
    "# Initialize train and test DataFrames\n",
    "train_input_data = qm_matrix_synthetic.copy()\n",
    "test_input_data = qm_matrix_synthetic.iloc[:, test_column_indices]\n",
    "\n",
    "# Drop the test columns from the train DataFrame\n",
    "train_input_data.drop(qm_matrix_synthetic.columns[test_column_indices], axis=1, inplace=True)\n",
    "first_column = train_input_data.iloc[:, 0]\n",
    "test_input_data.insert(0, 'Protein', first_column)\n",
    "\n",
    "train_patient_names = train_input_data.columns[1:]  # Exclude the first column 'Genes'\n",
    "test_patient_names = test_input_data.columns[1:]   # Exclude the first column 'Genes'\n",
    "\n",
    "# Subset design_matrix_synthetic for train and test\n",
    "train_design_matrix = design_matrix_synthetic[design_matrix_synthetic['sample'].isin(train_patient_names)]\n",
    "test_design_matrix = design_matrix_synthetic[design_matrix_synthetic['sample'].isin(test_patient_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "2    62\n",
       "1    38\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_design_matrix.value_counts(\"group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BINN is on the device: cpu\n",
      "dasdf\n",
      "Epoch 1/50, Loss: 0.6874, Accuracy: 0.6350\n",
      "Epoch 2/50, Loss: 0.5136, Accuracy: 0.7500\n",
      "Epoch 3/50, Loss: 0.4689, Accuracy: 0.7625\n",
      "Epoch 4/50, Loss: 0.4644, Accuracy: 0.7975\n",
      "Epoch 5/50, Loss: 0.4612, Accuracy: 0.7825\n",
      "Epoch 6/50, Loss: 0.4489, Accuracy: 0.7925\n",
      "Epoch 7/50, Loss: 0.4789, Accuracy: 0.7925\n",
      "Epoch 8/50, Loss: 0.4073, Accuracy: 0.8200\n",
      "Epoch 9/50, Loss: 0.4743, Accuracy: 0.7850\n",
      "Epoch 10/50, Loss: 0.4041, Accuracy: 0.8300\n",
      "Epoch 11/50, Loss: 0.4660, Accuracy: 0.7875\n",
      "Epoch 12/50, Loss: 0.4305, Accuracy: 0.7925\n",
      "Epoch 13/50, Loss: 0.4770, Accuracy: 0.7700\n",
      "Epoch 14/50, Loss: 0.4446, Accuracy: 0.7750\n",
      "Epoch 15/50, Loss: 0.4046, Accuracy: 0.8200\n",
      "Epoch 16/50, Loss: 0.4154, Accuracy: 0.8325\n",
      "Epoch 17/50, Loss: 0.4215, Accuracy: 0.8050\n",
      "Epoch 18/50, Loss: 0.4366, Accuracy: 0.8000\n",
      "Epoch 19/50, Loss: 0.4074, Accuracy: 0.8050\n",
      "Epoch 20/50, Loss: 0.4528, Accuracy: 0.7875\n",
      "Epoch 21/50, Loss: 0.4112, Accuracy: 0.8125\n",
      "Epoch 22/50, Loss: 0.3777, Accuracy: 0.8325\n",
      "Epoch 23/50, Loss: 0.3732, Accuracy: 0.8250\n",
      "Epoch 24/50, Loss: 0.4182, Accuracy: 0.8075\n",
      "Epoch 25/50, Loss: 0.3974, Accuracy: 0.8075\n",
      "Epoch 26/50, Loss: 0.4099, Accuracy: 0.8175\n",
      "Epoch 27/50, Loss: 0.4020, Accuracy: 0.8225\n",
      "Epoch 28/50, Loss: 0.4092, Accuracy: 0.8100\n",
      "Epoch 29/50, Loss: 0.4210, Accuracy: 0.8200\n",
      "Epoch 30/50, Loss: 0.4132, Accuracy: 0.8325\n",
      "Epoch 31/50, Loss: 0.3977, Accuracy: 0.8050\n",
      "Epoch 32/50, Loss: 0.4179, Accuracy: 0.8050\n",
      "Epoch 33/50, Loss: 0.4369, Accuracy: 0.7925\n",
      "Epoch 34/50, Loss: 0.3701, Accuracy: 0.8350\n",
      "Epoch 35/50, Loss: 0.3781, Accuracy: 0.8175\n",
      "Epoch 36/50, Loss: 0.3506, Accuracy: 0.8350\n",
      "Epoch 37/50, Loss: 0.3420, Accuracy: 0.8725\n",
      "Epoch 38/50, Loss: 0.3886, Accuracy: 0.8275\n",
      "Epoch 39/50, Loss: 0.3835, Accuracy: 0.8175\n",
      "Epoch 40/50, Loss: 0.3490, Accuracy: 0.8750\n",
      "Epoch 41/50, Loss: 0.4143, Accuracy: 0.7975\n",
      "Epoch 42/50, Loss: 0.3725, Accuracy: 0.8250\n",
      "Epoch 43/50, Loss: 0.3959, Accuracy: 0.8100\n",
      "Epoch 44/50, Loss: 0.3973, Accuracy: 0.8200\n",
      "Epoch 45/50, Loss: 0.3985, Accuracy: 0.8200\n",
      "Epoch 46/50, Loss: 0.4079, Accuracy: 0.8325\n",
      "Epoch 47/50, Loss: 0.3760, Accuracy: 0.8300\n",
      "Epoch 48/50, Loss: 0.3818, Accuracy: 0.8375\n",
      "Epoch 49/50, Loss: 0.4069, Accuracy: 0.8175\n",
      "Epoch 50/50, Loss: 0.3631, Accuracy: 0.8350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'or epoch in range(num_epochs):\\n    binn.train() \\n    total_loss = 0.0\\n    total_accuracy = 0\\n\\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\\n        inputs = inputs.to(binn.device)\\n        targets = targets.to(binn.device).type(torch.LongTensor)\\n        optimizer.zero_grad()\\n        outputs = binn(inputs).to(binn.device)\\n        loss = F.cross_entropy(outputs, targets)\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n        total_accuracy += torch.sum(torch.argmax(outputs, axis=1) == targets) / len(targets)\\n\\n    avg_loss = total_loss / len(dataloader)\\n    avg_accuracy = total_accuracy / len(dataloader)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the names o fhte data frames: qm_matrix_synthetic, design_matrix_synthetic, translations_synthetic, parent_child_df \n",
    "\n",
    "\n",
    "\n",
    "# Train a binn model on the normal data\n",
    "\n",
    "network = Network(\n",
    "        input_data    =   train_input_data,#qm_matrix_synthetic, # use the preprocessed data\n",
    "        pathways      =   parent_child_df,\n",
    "        mapping       =   translations_synthetic,\n",
    "        input_data_column = \"Protein\", # This is the default value\n",
    "        source_column = \"child\", # defined by our pathways-file\n",
    "        target_column = \"parent\",\n",
    "        subset_pathways = True # This is the default value\n",
    "    )\n",
    "\n",
    "binn = BINN(\n",
    "    network=network,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    validate=False,\n",
    "    residual=False,\n",
    "    device=\"cpu\",\n",
    "    learning_rate=0.001\n",
    "    # defaults\n",
    "    # activation = \"tanh\"\n",
    "    # weight = torch.tensor([1, 1])\n",
    "    # scheduler = \"plateau\"\n",
    "    # optimizer = \"adam\"\n",
    "    # n_outputs = 2\n",
    ")\n",
    "\n",
    "### Training with all train data no CV\n",
    "\n",
    "protein_matrix = fit_data_matrix_to_network_input(train_input_data, features=network.inputs)\n",
    "X, y = generate_data(protein_matrix, design_matrix=train_design_matrix)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X, dtype=torch.float32, device=binn.device),\n",
    "    torch.tensor(y, dtype=torch.torch.long, device=binn.device),\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# You can also train with a standard PyTorch train loop \n",
    "\n",
    "optimizer = binn.configure_optimizers()[0][0]\n",
    "\n",
    "num_epochs = 50\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    binn.train() \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(binn.device), targets.to(binn.device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = binn(inputs)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += (torch.argmax(outputs, axis=1) == targets).sum().item() / len(targets)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')\n",
    "\n",
    "\"\"\"or epoch in range(num_epochs):\n",
    "    binn.train() \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs = inputs.to(binn.device)\n",
    "        targets = targets.to(binn.device).type(torch.LongTensor)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = binn(inputs).to(binn.device)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += torch.sum(torch.argmax(outputs, axis=1) == targets) / len(targets)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / len(dataloader)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dasdf\n"
     ]
    }
   ],
   "source": [
    "# Test the Binn model on the \n",
    "\n",
    "# and test_loader is your DataLoader for the test data\n",
    "protein_matrix_test = fit_data_matrix_to_network_input(test_input_data, features=network.inputs)\n",
    "# Assuming generate_data() returns the entire dataset\n",
    "X_test, y_test = generate_data(protein_matrix_test, design_matrix=test_design_matrix)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # tryout long instead of int16\n",
    "\n",
    "# Create a DataLoader for your test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)  # Adjust batch size as needed\n",
    "# Set the model to evaluation mode\n",
    "binn.eval()\n",
    "\n",
    "# Initialize variables to track accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# No need to track gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move data to the same device as your model\n",
    "        inputs = inputs.to(binn.device)\n",
    "        labels = labels.to(binn.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = binn(inputs)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Total number of labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming train_input_data and train_design_matrix are your training data\n",
    "\n",
    "# Reshape train_input_data: drop 'Protein' column and transpose it\n",
    "X_train = train_input_data.drop('Protein', axis=1).T\n",
    "\n",
    "# Map 'sample' in train_design_matrix to match index of X_train\n",
    "y_train = train_design_matrix.set_index('sample').reindex(X_train.index)['group']\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.61      0.60        38\n",
      "           2       0.75      0.74      0.75        62\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.67      0.67      0.67       100\n",
      "weighted avg       0.69      0.69      0.69       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming test_input_data and test_design_matrix are your testing data\n",
    "\n",
    "# Reshape test_input_data: drop 'Protein' column and transpose it\n",
    "X_test = test_input_data.drop('Protein', axis=1).T\n",
    "\n",
    "# Map 'sample' in test_design_matrix to match index of X_test\n",
    "y_test = test_design_matrix.set_index('sample').reindex(X_test.index)['group']\n",
    "\n",
    "# Make predictions using the trained model\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dasdf\n"
     ]
    }
   ],
   "source": [
    "background_protein_matrix = fit_data_matrix_to_network_input(train_input_data, features=network.inputs)\n",
    "background_X, _ = generate_data(background_protein_matrix, design_matrix=train_design_matrix)\n",
    "\n",
    "background_data = torch.tensor(background_X, dtype=torch.float32, device=binn.device)\n",
    "# Use the same data for test data\n",
    "test_data = background_data.clone()  # Cloning to keep test data separate\n",
    "# Initialize the explainer with your trained model\n",
    "explainer = BINNExplainer(binn)\n",
    "\n",
    "# Generate explanations\n",
    "explanation_df = explainer.explain(test_data, background_data)\n",
    "\n",
    "# Analyze the explanation_df dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source name</th>\n",
       "      <th>target name</th>\n",
       "      <th>value</th>\n",
       "      <th>type</th>\n",
       "      <th>source layer</th>\n",
       "      <th>target layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>106</td>\n",
       "      <td>196</td>\n",
       "      <td>Gene_288</td>\n",
       "      <td>Level_3Pathway2</td>\n",
       "      <td>0.272471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>106</td>\n",
       "      <td>200</td>\n",
       "      <td>Gene_288</td>\n",
       "      <td>Level_3Pathway6</td>\n",
       "      <td>0.272471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>56</td>\n",
       "      <td>255</td>\n",
       "      <td>Gene_208</td>\n",
       "      <td>Level_1Pathway13</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>56</td>\n",
       "      <td>200</td>\n",
       "      <td>Gene_208</td>\n",
       "      <td>Level_3Pathway6</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>56</td>\n",
       "      <td>196</td>\n",
       "      <td>Gene_208</td>\n",
       "      <td>Level_3Pathway2</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>56</td>\n",
       "      <td>297</td>\n",
       "      <td>Gene_208</td>\n",
       "      <td>Level_1Pathway7</td>\n",
       "      <td>0.169622</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>89</td>\n",
       "      <td>242</td>\n",
       "      <td>Gene_262</td>\n",
       "      <td>Level_2Pathway2</td>\n",
       "      <td>0.159064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>89</td>\n",
       "      <td>268</td>\n",
       "      <td>Gene_262</td>\n",
       "      <td>Level_1Pathway25</td>\n",
       "      <td>0.159064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>297</td>\n",
       "      <td>Gene_108</td>\n",
       "      <td>Level_1Pathway7</td>\n",
       "      <td>0.152920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>71</td>\n",
       "      <td>298</td>\n",
       "      <td>Gene_237</td>\n",
       "      <td>Level_1Pathway8</td>\n",
       "      <td>0.151966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>71</td>\n",
       "      <td>283</td>\n",
       "      <td>Gene_237</td>\n",
       "      <td>Level_1Pathway39</td>\n",
       "      <td>0.151966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>47</td>\n",
       "      <td>276</td>\n",
       "      <td>Gene_192</td>\n",
       "      <td>Level_1Pathway32</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>47</td>\n",
       "      <td>279</td>\n",
       "      <td>Gene_192</td>\n",
       "      <td>Level_1Pathway35</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>47</td>\n",
       "      <td>291</td>\n",
       "      <td>Gene_192</td>\n",
       "      <td>Level_1Pathway46</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>47</td>\n",
       "      <td>293</td>\n",
       "      <td>Gene_192</td>\n",
       "      <td>Level_1Pathway48</td>\n",
       "      <td>0.132872</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "      <td>Gene_0</td>\n",
       "      <td>Level_2Pathway2</td>\n",
       "      <td>0.126012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>58</td>\n",
       "      <td>245</td>\n",
       "      <td>Gene_210</td>\n",
       "      <td>Level_2Pathway5</td>\n",
       "      <td>0.124702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>155</td>\n",
       "      <td>196</td>\n",
       "      <td>Gene_96</td>\n",
       "      <td>Level_3Pathway2</td>\n",
       "      <td>0.122177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>155</td>\n",
       "      <td>200</td>\n",
       "      <td>Gene_96</td>\n",
       "      <td>Level_3Pathway6</td>\n",
       "      <td>0.122177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>105</td>\n",
       "      <td>272</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_1Pathway29</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>105</td>\n",
       "      <td>194</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_3Pathway0</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>105</td>\n",
       "      <td>234</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_2Pathway12</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>105</td>\n",
       "      <td>293</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_1Pathway48</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>105</td>\n",
       "      <td>279</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_1Pathway35</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>105</td>\n",
       "      <td>262</td>\n",
       "      <td>Gene_287</td>\n",
       "      <td>Level_1Pathway2</td>\n",
       "      <td>0.118042</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>88</td>\n",
       "      <td>297</td>\n",
       "      <td>Gene_261</td>\n",
       "      <td>Level_1Pathway7</td>\n",
       "      <td>0.117901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>88</td>\n",
       "      <td>195</td>\n",
       "      <td>Gene_261</td>\n",
       "      <td>Level_3Pathway1</td>\n",
       "      <td>0.117901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>91</td>\n",
       "      <td>242</td>\n",
       "      <td>Gene_266</td>\n",
       "      <td>Level_2Pathway2</td>\n",
       "      <td>0.112804</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>91</td>\n",
       "      <td>241</td>\n",
       "      <td>Gene_266</td>\n",
       "      <td>Level_2Pathway19</td>\n",
       "      <td>0.112804</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>35</td>\n",
       "      <td>194</td>\n",
       "      <td>Gene_162</td>\n",
       "      <td>Level_3Pathway0</td>\n",
       "      <td>0.104621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>35</td>\n",
       "      <td>270</td>\n",
       "      <td>Gene_162</td>\n",
       "      <td>Level_1Pathway27</td>\n",
       "      <td>0.104621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>35</td>\n",
       "      <td>234</td>\n",
       "      <td>Gene_162</td>\n",
       "      <td>Level_2Pathway12</td>\n",
       "      <td>0.104621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>112</td>\n",
       "      <td>250</td>\n",
       "      <td>Gene_296</td>\n",
       "      <td>Level_1Pathway0</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>112</td>\n",
       "      <td>241</td>\n",
       "      <td>Gene_296</td>\n",
       "      <td>Level_2Pathway19</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>41</td>\n",
       "      <td>272</td>\n",
       "      <td>Gene_179</td>\n",
       "      <td>Level_1Pathway29</td>\n",
       "      <td>0.104106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>142</td>\n",
       "      <td>198</td>\n",
       "      <td>Gene_7</td>\n",
       "      <td>Level_3Pathway4</td>\n",
       "      <td>0.100722</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>27</td>\n",
       "      <td>248</td>\n",
       "      <td>Gene_150</td>\n",
       "      <td>Level_2Pathway8</td>\n",
       "      <td>0.094765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "      <td>Gene_150</td>\n",
       "      <td>Level_1Pathway3</td>\n",
       "      <td>0.094765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>27</td>\n",
       "      <td>202</td>\n",
       "      <td>Gene_150</td>\n",
       "      <td>Level_3Pathway8</td>\n",
       "      <td>0.094765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>27</td>\n",
       "      <td>281</td>\n",
       "      <td>Gene_150</td>\n",
       "      <td>Level_1Pathway37</td>\n",
       "      <td>0.094765</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source  target source name       target name     value  type  \\\n",
       "340     106     196    Gene_288   Level_3Pathway2  0.272471     0   \n",
       "342     106     200    Gene_288   Level_3Pathway6  0.272471     0   \n",
       "172      56     255    Gene_208  Level_1Pathway13  0.169622     0   \n",
       "178      56     200    Gene_208   Level_3Pathway6  0.169622     0   \n",
       "176      56     196    Gene_208   Level_3Pathway2  0.169622     0   \n",
       "174      56     297    Gene_208   Level_1Pathway7  0.169622     0   \n",
       "277      89     242    Gene_262   Level_2Pathway2  0.159064     1   \n",
       "275      89     268    Gene_262  Level_1Pathway25  0.159064     1   \n",
       "12        6     297    Gene_108   Level_1Pathway7  0.152920     0   \n",
       "216      71     298    Gene_237   Level_1Pathway8  0.151966     0   \n",
       "214      71     283    Gene_237  Level_1Pathway39  0.151966     0   \n",
       "146      47     276    Gene_192  Level_1Pathway32  0.132872     0   \n",
       "148      47     279    Gene_192  Level_1Pathway35  0.132872     0   \n",
       "150      47     291    Gene_192  Level_1Pathway46  0.132872     0   \n",
       "152      47     293    Gene_192  Level_1Pathway48  0.132872     0   \n",
       "1         1     242      Gene_0   Level_2Pathway2  0.126012     1   \n",
       "182      58     245    Gene_210   Level_2Pathway5  0.124702     0   \n",
       "484     155     196     Gene_96   Level_3Pathway2  0.122177     0   \n",
       "486     155     200     Gene_96   Level_3Pathway6  0.122177     0   \n",
       "330     105     272    Gene_287  Level_1Pathway29  0.118042     0   \n",
       "338     105     194    Gene_287   Level_3Pathway0  0.118042     0   \n",
       "336     105     234    Gene_287  Level_2Pathway12  0.118042     0   \n",
       "334     105     293    Gene_287  Level_1Pathway48  0.118042     0   \n",
       "332     105     279    Gene_287  Level_1Pathway35  0.118042     0   \n",
       "328     105     262    Gene_287   Level_1Pathway2  0.118042     0   \n",
       "270      88     297    Gene_261   Level_1Pathway7  0.117901     0   \n",
       "272      88     195    Gene_261   Level_3Pathway1  0.117901     0   \n",
       "283      91     242    Gene_266   Level_2Pathway2  0.112804     1   \n",
       "281      91     241    Gene_266  Level_2Pathway19  0.112804     1   \n",
       "116      35     194    Gene_162   Level_3Pathway0  0.104621     0   \n",
       "112      35     270    Gene_162  Level_1Pathway27  0.104621     0   \n",
       "114      35     234    Gene_162  Level_2Pathway12  0.104621     0   \n",
       "361     112     250    Gene_296   Level_1Pathway0  0.104574     1   \n",
       "363     112     241    Gene_296  Level_2Pathway19  0.104574     1   \n",
       "132      41     272    Gene_179  Level_1Pathway29  0.104106     0   \n",
       "446     142     198      Gene_7   Level_3Pathway4  0.100722     0   \n",
       "89       27     248    Gene_150   Level_2Pathway8  0.094765     1   \n",
       "81       27     273    Gene_150   Level_1Pathway3  0.094765     1   \n",
       "91       27     202    Gene_150   Level_3Pathway8  0.094765     1   \n",
       "83       27     281    Gene_150  Level_1Pathway37  0.094765     1   \n",
       "\n",
       "     source layer  target layer  \n",
       "340             0             1  \n",
       "342             0             1  \n",
       "172             0             1  \n",
       "178             0             1  \n",
       "176             0             1  \n",
       "174             0             1  \n",
       "277             0             1  \n",
       "275             0             1  \n",
       "12              0             1  \n",
       "216             0             1  \n",
       "214             0             1  \n",
       "146             0             1  \n",
       "148             0             1  \n",
       "150             0             1  \n",
       "152             0             1  \n",
       "1               0             1  \n",
       "182             0             1  \n",
       "484             0             1  \n",
       "486             0             1  \n",
       "330             0             1  \n",
       "338             0             1  \n",
       "336             0             1  \n",
       "334             0             1  \n",
       "332             0             1  \n",
       "328             0             1  \n",
       "270             0             1  \n",
       "272             0             1  \n",
       "283             0             1  \n",
       "281             0             1  \n",
       "116             0             1  \n",
       "112             0             1  \n",
       "114             0             1  \n",
       "361             0             1  \n",
       "363             0             1  \n",
       "132             0             1  \n",
       "446             0             1  \n",
       "89              0             1  \n",
       "81              0             1  \n",
       "91              0             1  \n",
       "83              0             1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explanation_df.sort_values(by = \"value\", ascending = False).head(20)\n",
    "\n",
    "explanation_df_sorted_filtered = explanation_df[explanation_df['source name'].str.contains(\"Gene\")]\n",
    "explanation_df_sorted_filtered.sort_values(by = \"value\", ascending = False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the key pathways are {'Level_1Pathway8', 'Level_0Pathway19', 'Level_1Pathway19', 'Level_2Pathway2', 'Level_0Pathway14', 'Level_3Pathway6', 'Level_3Pathway7', 'Level_1Pathway0', 'Level_0Pathway18', 'Level_2Pathway8', 'Level_0Pathway15', 'Level_3Pathway0', 'Level_2Pathway18', 'Level_0Pathway10', 'Level_2Pathway5', 'Level_3Pathway3', 'Level_2Pathway11', 'Level_2Pathway12', 'Level_0Pathway9', 'Level_1Pathway5', 'Level_0Pathway0', 'Level_1Pathway11', 'Level_1Pathway3', 'Level_1Pathway14', 'Level_1Pathway1', 'Level_0Pathway6', 'Level_1Pathway17', 'Level_1Pathway16', 'Level_0Pathway3', 'Level_0Pathway1', 'Level_3Pathway1', 'Level_2Pathway6', 'Level_1Pathway7', 'Level_2Pathway3', 'Level_1Pathway10', 'Level_1Pathway9', 'Level_0Pathway5', 'Level_3Pathway4', 'Level_1Pathway6', 'Level_0Pathway8', 'Level_1Pathway12', 'Level_2Pathway1', 'Level_2Pathway13', 'Level_3Pathway8', 'Level_0Pathway13', 'Level_2Pathway19', 'Level_2Pathway16', 'Level_2Pathway17', 'Level_3Pathway9', 'Level_0Pathway7', 'Level_2Pathway14', 'Level_2Pathway0', 'Level_2Pathway7', 'Level_0Pathway16', 'Level_1Pathway4', 'Level_2Pathway10', 'Level_0Pathway12', 'Level_1Pathway13', 'Level_0Pathway2', 'Level_0Pathway17', 'Level_2Pathway4', 'Level_2Pathway15', 'Level_3Pathway2', 'Level_1Pathway18', 'Level_1Pathway15', 'Level_2Pathway9', 'Level_0Pathway4', 'Level_1Pathway2', 'Level_3Pathway5', 'Level_0Pathway11'}\n",
      "the key genes are [ 34  50  65  82  92  97 104 112 157 162 174 185 188 210 217 218 226 271\n",
      " 278 288]\n"
     ]
    }
   ],
   "source": [
    "key_genes.sort()\n",
    "print(f\"the key pathways are {key_pathways}\")\n",
    "print(f\"the key genes are {key_genes}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source name\n",
       "Level_2Pathway16     0.210095\n",
       "Level_3Pathway2      0.174580\n",
       "Level_1Pathway9      0.157260\n",
       "Level_1Pathway7      0.148311\n",
       "Gene_288             0.144112\n",
       "Level_2Pathway2      0.135746\n",
       "Level_0Pathway130    0.124500\n",
       "Level_2Pathway5      0.117403\n",
       "Level_2Pathway14     0.113414\n",
       "Gene_192             0.112957\n",
       "Gene_108             0.111557\n",
       "Level_3Pathway6      0.110981\n",
       "Level_1Pathway45     0.109585\n",
       "Level_0Pathway88     0.101156\n",
       "Level_0Pathway8      0.100119\n",
       "Gene_208             0.099973\n",
       "Level_1Pathway48     0.099973\n",
       "Level_0Pathway37     0.099930\n",
       "Level_1Pathway37     0.099861\n",
       "Level_0Pathway63     0.099059\n",
       "Gene_237             0.096050\n",
       "Level_0Pathway67     0.094924\n",
       "Level_3Pathway4      0.094675\n",
       "Gene_261             0.094129\n",
       "Level_1Pathway42     0.093785\n",
       "Level_0Pathway31     0.093721\n",
       "Level_0Pathway144    0.092887\n",
       "Level_1Pathway17     0.091747\n",
       "Gene_210             0.090880\n",
       "Gene_262             0.090285\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, compute the absolute values of the SHAP values\n",
    "\n",
    "\n",
    "# Then, group by 'source name' and calculate the mean of the absolute values\n",
    "mean_abs_shap_by_feature = explanation_df.groupby('source name')['value'].mean().sort_values(ascending=False)\n",
    "\"\"\"mean_abs_shap_by_feature_filtered = mean_abs_shap_by_feature[mean_abs_shap_by_feature['source name'].str.contains(\"Gene\")]\n",
    "mean_abs_shap_by_feature_filtered.sort_values(by = \"value\", ascending = False).head(40)\"\"\"\n",
    "\n",
    "\n",
    "# # Select the top N features\n",
    "# num_top_features = 10  # for example\n",
    "# top_features = mean_abs_shap_by_feature.head(num_top_features).index.tolist()\n",
    "\n",
    "# # Extract SHAP values for the top features\n",
    "# top_features_shap_values = explanation_df[explanation_df['source name'].isin(top_features)]\n",
    "mean_abs_shap_by_feature.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abs_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46644\\585963597.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexplanation_df_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplanation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"abs_value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#[  0  15  25  26  44  59  61  63  97 117 124 134 138 156 157 158 176 177 188 199 212 213 215 233 238 241 273 276 278 293]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexplanation_df_sorted_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplanation_df_sorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexplanation_df_sorted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'source name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gene\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mexplanation_df_sorted_filtered\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\anaconda3\\envs\\binni\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6940\u001b[0m             )\n\u001b[0;32m   6941\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6942\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6944\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6946\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6947\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\frede\\anaconda3\\envs\\binni\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'abs_value'"
     ]
    }
   ],
   "source": [
    "explanation_df_sorted = explanation_df.sort_values(by = \"abs_value\", ascending = True)\n",
    "#[  0  15  25  26  44  59  61  63  97 117 124 134 138 156 157 158 176 177 188 199 212 213 215 233 238 241 273 276 278 293]\n",
    "explanation_df_sorted_filtered = explanation_df_sorted[explanation_df_sorted['source name'].str.contains(\"Gene\")]\n",
    "explanation_df_sorted_filtered.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, compute the absolute values of the SHAP values\n",
    "explanation_df['abs_value'] = explanation_df['value'].abs()\n",
    "\n",
    "# Then, group by 'source name' and calculate the mean of the absolute values\n",
    "mean_abs_shap_by_feature = explanation_df.groupby('source name')['abs_value'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Select the top N features\n",
    "num_top_features = 10  # for example\n",
    "top_features = mean_abs_shap_by_feature.head(num_top_features).index.tolist()\n",
    "\n",
    "# Extract SHAP values for the top features\n",
    "top_features_shap_values = explanation_df[explanation_df['source name'].isin(top_features)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source name  abs_value  rank  source layer\n",
      "0    Gene_171   0.163694   1.0             0\n",
      "1    Gene_269   0.111017   2.0             0\n",
      "2     Gene_66   0.110108   3.0             0\n",
      "3    Gene_217   0.102805   4.0             0\n",
      "4     Gene_11   0.096489   5.0             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming explanation_df is your dataframe and binn is your trained BINN model\n",
    "n_layers = binn.n_layers  # Number of layers in the BINN model\n",
    "\n",
    "# Initialize an empty dataframe for the final results\n",
    "plot_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each layer\n",
    "for layer in range(n_layers):\n",
    "    # Filter the dataframe for the current layer\n",
    "    layer_df = explanation_df[explanation_df['source layer'] == layer]\n",
    "\n",
    "    # Group by 'source name', then calculate the mean SHAP value\n",
    "    grouped_df = layer_df.groupby('source name', as_index=False)['abs_value'].mean()\n",
    "\n",
    "    # Sort the grouped dataframe based on mean SHAP values\n",
    "    grouped_df.sort_values(by='abs_value', ascending=False, inplace=True)\n",
    "\n",
    "    # Rank the features based on their mean SHAP values\n",
    "    grouped_df['rank'] = grouped_df['abs_value'].rank(ascending=False)\n",
    "\n",
    "    # Add a column for the layer number\n",
    "    grouped_df['source layer'] = layer\n",
    "\n",
    "    # Append to the final dataframe\n",
    "    plot_df = pd.concat([plot_df, grouped_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(plot_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233 119  58 106 177 270  77  10 257  17 203   2 130 134  84 101  13 225\n",
      "  31  23  30 181 212 195   8 224  28 213  26 299]\n",
      "{'Level_1Pathway11', 'Level_0Pathway2', 'Level_3Pathway3', 'Level_3Pathway4', 'Level_3Pathway7', 'Level_1Pathway9', 'Level_2Pathway12', 'Level_0Pathway13', 'Level_2Pathway2', 'Level_0Pathway12', 'Level_0Pathway4', 'Level_0Pathway1', 'Level_1Pathway2', 'Level_0Pathway14', 'Level_3Pathway9', 'Level_1Pathway10', 'Level_1Pathway4', 'Level_2Pathway4', 'Level_0Pathway5', 'Level_0Pathway9', 'Level_0Pathway8', 'Level_2Pathway13', 'Level_1Pathway12', 'Level_3Pathway1', 'Level_2Pathway10', 'Level_1Pathway0', 'Level_3Pathway6', 'Level_2Pathway9', 'Level_1Pathway8', 'Level_3Pathway0', 'Level_0Pathway11', 'Level_0Pathway3', 'Level_1Pathway13', 'Level_2Pathway5', 'Level_0Pathway6', 'Level_3Pathway2', 'Level_1Pathway1', 'Level_1Pathway6', 'Level_0Pathway7', 'Level_3Pathway5', 'Level_2Pathway0', 'Level_3Pathway8', 'Level_0Pathway0', 'Level_2Pathway7', 'Level_1Pathway5', 'Level_2Pathway6', 'Level_1Pathway7', 'Level_0Pathway10', 'Level_2Pathway1', 'Level_2Pathway11', 'Level_2Pathway14', 'Level_2Pathway3', 'Level_1Pathway3', 'Level_2Pathway8', 'Level_1Pathway14'}\n"
     ]
    }
   ],
   "source": [
    "print(key_genes)\n",
    "print(key_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gene_0',\n",
       " 'Gene_15',\n",
       " 'Gene_25',\n",
       " 'Gene_26',\n",
       " 'Gene_44',\n",
       " 'Gene_59',\n",
       " 'Gene_61',\n",
       " 'Gene_63',\n",
       " 'Gene_97',\n",
       " 'Gene_117',\n",
       " 'Gene_124',\n",
       " 'Gene_134',\n",
       " 'Gene_138',\n",
       " 'Gene_156',\n",
       " 'Gene_157',\n",
       " 'Gene_158',\n",
       " 'Gene_176',\n",
       " 'Gene_177',\n",
       " 'Gene_188',\n",
       " 'Gene_199',\n",
       " 'Gene_212',\n",
       " 'Gene_213',\n",
       " 'Gene_215',\n",
       " 'Gene_233',\n",
       " 'Gene_238',\n",
       " 'Gene_241',\n",
       " 'Gene_273',\n",
       " 'Gene_276',\n",
       " 'Gene_278',\n",
       " 'Gene_293']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "key_genes_sorted = key_genes\n",
    "key_genes_gene = [f\"Gene_{i}\" for i in key_genes_sorted]\n",
    "key_genes_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Level_1Pathway0\" in key_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_abs_shap_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Select the top 50 features from mean_abs_shap_filtered\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m top_50_features \u001b[38;5;241m=\u001b[39m mean_abs_shap_filtered\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check how many key genes are in the top 50 features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(gene \u001b[38;5;129;01min\u001b[39;00m top_50_features \u001b[38;5;28;01mfor\u001b[39;00m gene \u001b[38;5;129;01min\u001b[39;00m key_genes_gene)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_abs_shap_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the top 50 features from mean_abs_shap_filtered\n",
    "top_50_features = mean_abs_shap_filtered.head(50).index.tolist()\n",
    "\n",
    "# Check how many key genes are in the top 50 features\n",
    "matches = sum(gene in top_50_features for gene in key_genes_gene)\n",
    "\n",
    "print(f\"Number of key genes in the top 50 features: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source name\n",
       "Gene_171             0.163694\n",
       "Level_1Pathway11     0.153358\n",
       "Level_2Pathway19     0.152273\n",
       "Level_0Pathway38     0.148235\n",
       "Level_0Pathway34     0.146577\n",
       "                       ...   \n",
       "Gene_179             0.000027\n",
       "Gene_78              0.000027\n",
       "Gene_116             0.000004\n",
       "Level_0Pathway134    0.000003\n",
       "Gene_274             0.000001\n",
       "Name: abs_value, Length: 286, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_abs_shap_by_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
